# --------------------------------------------------------------------------------------------------------
# Course number (i.e. IST 687) 
# Alex Bernal
# FINAL Project
# 9/20/ 2018 
# pages
# Dataset - Bike Share Washinton DC
# Agile Kaban Methodology
# Project Summary
# 3 Methods of Modeling to try and predict Bike Demand
# Linear Regression, Multilinear Regression and Seasonality forecast
# Goals : Find Latent Patterns and groups, Evaluate accuracy
# --------------------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------------------
# Step 1: load data


# Start by setting your working directory.
setwd("C:\\Users\\ObsidianQuantitative\\Desktop\\Syracuse classes\\IST687_Intro_to_DataScience")
bike = bike.data

# What does our data look like?

# I first want to isolate data to exclude variability based on 
# hour of the day and weekends and holidays. Let's subset data only for weekdays at noon.
bike_weekday_noon = subset(bike, 
                           `Working.Day` == 1 & Hour == 12)  #<- criteria for subsetting
View(bike_weekday_noon)

# --------------------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------------------

#: Step 1 : plot the data

# Hypothesis : temperature influences bike rentals.
# plot temperature vs. the total count of bike rentals.
library(ggplot2)

# Display the column names so the code is easier to write.
colnames(bike_weekday_noon)

# linear regression line ####
ggplot(bike_weekday_noon,                                      
       aes(x = `Temperature.F`,                                
           y = `Total.Users`)) +                            
  geom_point() +                                               
  stat_smooth(method = "lm",       #<- use linear regression
              se = FALSE,          #<- don't plot the confidence interval, we'll discuss this later
              color = "orange",    #<- make the best fit line orange
              size = 3) +          #<- set the thickness of the best-fit line
  expand_limits(x = 0, y = 0) +                                
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +   #set scal to y axis      
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +         
  theme(text = element_text(size = 20),                     
        panel.border = element_rect(color = "black",        
                                    fill = NA,                 
                                    size = 1),                 
        panel.background = element_rect(fill = "white"),       
        panel.grid.minor = element_line(color = NA),           #Remove minor gridlines         
        panel.grid.major = element_line(color = "grey",        #set to grey
                                        linetype = "dashed"))  

# --------------------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------------------

#S Run linear regression ####

#linear regression model of temperature vs. number of bike rentals

lm_weekday_noon_temp = lm(formula = `Total.Users` ~ `Temperature.F`,  data = bike_weekday_noon)                   


# Check the output of the linear model.
lm_weekday_noon_temp
summary(lm_weekday_noon_temp)

# interpret the intercept and coefficient
# For a 1 degree increase in temp, we expect 2 (2.589) more rentals.
# At 0 degrees, we expect 37 (37.596) bike rentals.

# The linear regression model objects check
ls(lm_weekday_noon_temp)


# --------------------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------------------

#Model accuracy

####  Plot the residuals ####

# Create a new variable to represent residuals. Cast is as a data frame
# so that ggplot knows what to plot on the x-axis.
lm_weekday_noon_temp_resid = as.data.frame(lm_weekday_noon_temp$residuals)

# Plot the distribution of the residuals.
ggplot(lm_weekday_noon_temp_resid, 
       aes(x = lm_weekday_noon_temp_resid[, 1])) + 
  geom_histogram(color = "white",      
                 fill = "brown",       
                 binwidth = 25) +      #<- each bar should encompass 25 units
  scale_y_continuous(breaks = seq(0, 120, by = 10)) +
  scale_x_continuous(breaks = seq(-200, 300, by = 50)) +
  theme(text = element_text(size = 20),                     
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed")) + 
  labs(y = "Frequency",         #<- y-axis label
       x = "Residuals")         #<- x-axis label

# this is a visual check that the variables are normally distributed

## Calculate the variance of residuals
# The difference between the expected value and the actual value is the residual

# The var() function produces the variance of a data set.
var(lm_weekday_noon_temp$residuals)

# Manual Check
resid_avg = mean(lm_weekday_noon_temp_resid[, 1])
resid_avg

# Create a loop to subtract the average from each residual value and square
# the result. Start with a blank data frame.
resid_less_avg = data.frame(nrow = nrow(lm_weekday_noon_temp_resid),  #<- number of rows in the blank data frame
                            ncol = 1)                                 #<- number of columns in the blank data frame

# Run a for() to do this quickly.
for(i in 1:nrow(lm_weekday_noon_temp_resid)){
  resid_less_avg[i] = (lm_weekday_noon_temp_resid[i, 1] - resid_avg) ^ 2
}

# View the output.
View(resid_less_avg) #squared error for each datapoint

# number of data points - 1 in the denominator. 
# calculating covariance.
variance_check = sum(resid_less_avg) / (nrow(lm_weekday_noon_temp_resid) - 1)
variance_check

###Standard deviation of the residuals ####

# The sd() function calculates the standard deviation.
sd(lm_weekday_noon_temp$residuals)

# Take the square root of variance
(var(lm_weekday_noon_temp$residuals) ^ (1 / 2))
sqrt(var(lm_weekday_noon_temp$residuals))

##Understanding residuals ####

# Look at the standard deviation for the total number of users and temperature.
sd(bike_weekday_noon$`Total.Users`) #89.26
sd(bike_weekday_noon$`Temperature.F`) #16.94

# Test normality of residual distribution ####

# Plot a quantile-quantile plot (Q-Q plot) to test normality of distribution
# of the data.
qqnorm(lm_weekday_noon_temp$residuals,
       main = "Q-Q Plot to Test Normality of Residuals")

qqline(lm_weekday_noon_temp$residuals,  #<- data to use
       col = "red",                     #<- color of the line to use
       lwd = 2,                         #<- thickness of the line
       probs = c(0.25, 0.75))           #<- percentiles to draw the line through   

# this Q-Q plot compares 2 distributions to determine if there is a normal distribution
# the closer the points are to the linear line the more simular the distributions are

#Plot Standard Error Line : so it can show us the confidence interval of a regression line,
# with X% certainty where the best fit line should be.

ggplot(bike_weekday_noon, 
       aes(x = `Temperature.F`,
           y = `Total.Users`)) +
  geom_point() +
  stat_smooth(method = "lm",       
              #se = FALSE,         
              color = "orange",    
              size = 1,            
              level = 0.95) +      #<- 95% degree of confidence 
  expand_limits(x = 0, y = 0) +
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  theme(text = element_text(size = 20),                     
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed")) 


#based on std of data the confident interval is the shade zone of where the line should fall
# this predicts potention zone if you had more out of sample data
# --------------------------------------------------------------------------------------------------------

#outliers : Cooks distance 
# Let's see if our Capital Bikeshare model comparing total number of users
# at noon on weekdays and temperature has any outliers.
#box plot.
View(bike_weekday_noon)
bike_weekday_noon_boxplot_data = bike_weekday_noon[, c("Temperature.F", "Total.Users")]
head(bike_weekday_noon_boxplot_data, 2)

boxplot(bike_weekday_noon_boxplot_data, main = "Capital Bikeshare")

bike_weekday_noon_boxplot = boxplot(bike_weekday_noon_boxplot_data, 
                                    main = "Capital Bikeshare")
bike_weekday_noon_boxplot


# --------------------------------------------------------------------------------------------------------
##
# Cook's distance.
bike_weekday_noon_CD = cooks.distance(lm_weekday_noon_temp)
bike_weekday_noon_CD_select = bike_weekday_noon_CD[bike_weekday_noon_CD > 
                                                     4/nrow(bike_weekday_noon)]
bike_weekday_noon_CD_select

## View the points highlighted by Cook's distance.
ggplot(bike_weekday_noon, 
       aes(x = `Temperature.F`,
           y = `Total.Users`)) +
  geom_point() +
  geom_point(data = bike_weekday_noon[c("397", "3711", "3735", "4743", "4767",
                                        "4839", "4935", "5007", "5583", "10618",
                                        "14480", "14816", "15320", "16459", "17248"), ],  #<- select the row names from cook's distance test
             aes(x = `Temperature.F`, y = `Total.Users`),  #<- define the x and y axes for the selected points
             color = "red",                                
             size = 3) +                                   
  stat_smooth(method = "lm",
              #se = FALSE,  
              color = "orange", 
              size = 1,         
              level = 0.95) + 
  expand_limits(x = 0, y = 0) +
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  theme(text = element_text(size = 20),                     
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed")) 

## 
# --------------------------------------------------------------------------------------------------------
# --------------------------------------------------------------------------------------------------------
##Cook's distance: remove outliers ####


bike_weekday_noon_clean = subset(bike_weekday_noon, 
                                 !(`Total.Users` %in%                 #<- the column that includes the subset criteria
                                     bike_weekday_noon_boxplot$out))  #<- list of outlier values

View(bike_weekday_noon_clean)
# Let's re-run the regression without these 2 data points.
lm_bike_weekday_noon_clean = lm(`Total.Users` ~ `Temperature.F`,
                                bike_weekday_noon_clean)

summary(lm_bike_weekday_noon_clean)

##Regression without outliers ####

ggplot(bike_weekday_noon_clean, 
       aes(x = `Temperature.F`,
           y = `Total.Users`)) +
  geom_point() +
  stat_smooth(method = "lm",       #<- use linear regression
              #se = FALSE,         #<- when this term is not present, R plots the standard error of the best fit line
              color = "orange",    
              size = 1,            
              level = 0.95) +      #<- degree of confidence (related to standard deviation concept)
  expand_limits(x = 0, y = 0) +
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  theme(text = element_text(size = 20),                     
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed")) 


#==================================================================================

#### Covariance and correlation Analysis####
#strength in relationships of variables

# Calculate covariance.
cov(bike_weekday_noon_clean$`Temperature.F`,  #<- first variable
    bike_weekday_noon_clean$`Total.Users`)    #<- second variable
#732.98 Covariance

# Calculate correlation.
cor(bike_weekday_noon_clean$`Temperature.F`,  #<- first variable
    bike_weekday_noon_clean$`Total.Users`)    #<- second variable
#0.49 Correlation increase of 49% positive relationship
# correlation does not imply causation

#==================================================================================

#### Calculating R^2 ####

# R^2 = 1 - randomness / variance
# R^2 = 1 - ((actual value - estimated value)^2 / 
#            (actual value - average value)^2) 

actual_values = bike_weekday_noon_clean$`Total.Users`
estimated_values = lm_bike_weekday_noon_clean$fitted.values

#calculate Randomness
bike_weekday_noon_randomness = sum((actual_values - estimated_values)^2)
bike_weekday_noon_randomness

#calculate variance
bike_weekday_noon_variance = sum((actual_values - mean(actual_values))^2)
bike_weekday_noon_variance

R_squared = 1 - bike_weekday_noon_randomness / bike_weekday_noon_variance
R_squared

#0.24 R Squared is the change in users by changein tempurature

summary(lm_bike_weekday_noon_clean)



#### Calculate the F-statistic ####

# F-statistic = (variance of the output variable / unexplained squared errors) *
#               (degrees of freedom / number of independent variables)

num_coefficients = length(lm_bike_weekday_noon_clean$coefficients)
num_coefficients
#2

data_pts_weekday_noon = nrow(bike_weekday_noon_clean)
data_pts_weekday_noon
#495

degrees_of_freedom = data_pts_weekday_noon - num_coefficients
degrees_of_freedom
#493 

F_stat = ((bike_weekday_noon_variance - bike_weekday_noon_randomness) / 
            bike_weekday_noon_randomness) *
  (degrees_of_freedom / (num_coefficients - 1))
F_stat
#157.70

####Calculate the p-value for F-statistic ####

# Now let's calculate the p-value of the F-statistic using the F-distribution.
p_val_F_stat = 1 - pf(F_stat,                      #<- the F-statistic we calculated
                      df1 = num_coefficients - 1,  #<- number of model coefficients excluding the intercept
                      df2 = degrees_of_freedom)    #<- degrees of freedom of the model
p_val_F_stat

# The p-value is 0, meaning that there is no chance the model or relationships 
# could have been found by random chance if the data is normally distributed.
# Note that R actually produces a p-value of 2.2e-16. This is equivalent to a
# decimal followed by 15 zeros and two 2's:
##2.2e-16 == 0.00000000000000022
##0.00319 % chance that the pvalue was discovered randomly


# Compare the F-statistic and p-value we calculated to what 
# R calculates with the lm() function.
summary(lm_bike_weekday_noon_clean)

#==================================================================================


#Include several variables, 10 different variables


#### Linear regression outputs ####

#output of the regression check
summary(lm_bike_weekday_noon_clean)

#### View correlations in the data ####

# View the data.
View(bike)
str(bike)

# Let's visualize all the correlations to see which ones might have more predictive value
install.packages("GGally")
library(GGally)

# rename the columns
bike_correlations = bike
colnames(bike_correlations) = c("Date", "Season", "Hour", "Holiday",
                                "Day_Week", "Work_Day", "Weather", "TemP_F",
                                "Temp_Feels_F", "Humidity", "Wind_Speed", "Cas_Users",
                                "Reg_Users", "Total_Users")

# save it directly to a pdf file
pdf("Bike Users Correlations_Total.pdf", width = 18, height = 18)

ggpairs(bike_correlations[, 2:14],  
        lower = list(continuous = "smooth"),  #<- the lower argument plots a best fit line using the parameter "smooth"
        diag = list(continuous = "bar"))      #<- the diag argument affects the distribution plots along the diagonal, we set them to be bars instead of the default density distributions

# complete the saving 
dev.off()

##View correlations in the data: at noon ####

# Let's subset the data to look at users only at noon on work days.
View(bike_weekday_noon)
bike_correlations_weekday_noon = bike_weekday_noon
colnames(bike_correlations_weekday_noon) = c("Date", "Season", "Hour", "Holiday",
                                             "Day_Week", "Work_Day", "Weather", "TemP_F",
                                             "Temp_Feels_F", "Humidity", "Wind_Speed", "Cas_Users",
                                             "Reg_Users", "Total_Users")

# Save the ggpairs() output directly to a pdf file.
pdf("Bike Users Correlations_Weekday_Noon.pdf", width = 14, height = 14)

ggpairs(bike_correlations_weekday_noon[, c(2, 5, 7:14)], 
        lower = list(continuous = "smooth"), 
        diag = list(continuous = "bar"))

# complete the saving process.
dev.off()


#==================================================================================
#### Test 2 : Total bike rentals vs. humidity ####

# correlation matrix.

correlations = cor(bike[-1])
View(correlations)

# correlation matrix w "corrplot" package.
install.packages("corrplot")
library(corrplot)

corrplot(correlations,       #<- the correlation matrix to use
         method = "number")  #<- the type of output to display in the grid

# Let's try another visualization.
corrplot(correlations, method = "circle")


# only measurements taken at noon during 
# the weekdays to exclude the effects of temporal variables.
# plot total bike rentals vs. humidity.
library(ggplot2)
ggplot(bike_weekday_noon, 
       aes(x = Humidity,
           y = `Total.Users`)) +
  geom_point() +
  stat_smooth(method = "lm",
              se = FALSE,
              color = "orange",
              size = 2) +
  expand_limits(x = 0, y = 0) +
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  theme(text = element_text(size = 20),
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed")) + 
  labs(x = "Humidity %")

#==================================================================================

#### Bike rentals vs. humidity: outliers ####

# test the data for outliers before we plot our regression.
bike_weekday_noon_boxplot_data_2 = bike_weekday_noon[, c("Humidity", "Total.Users")]
head(bike_weekday_noon_boxplot_data_2, 2)

boxplot(bike_weekday_noon_boxplot_data_2, main = "Capital.Bikeshare")

bike_weekday_noon_boxplot_2 = boxplot(bike_weekday_noon_boxplot_data_2, 
                                      main = "Capital.Bikeshare")
bike_weekday_noon_boxplot_2

# Cook's distance. 
lm_bike_weekday_noon_hum = lm(formula = `Total.Users` ~ Humidity, data = bike_weekday_noon)

bike_weekday_noon_CD_hum = cooks.distance(lm_bike_weekday_noon_hum)
bike_weekday_noon_CD_hum_select = bike_weekday_noon_CD_hum[bike_weekday_noon_CD_hum > 
                                                             4/nrow(bike_weekday_noon)]
bike_weekday_noon_CD_hum_select
names(bike_weekday_noon_CD_hum_select)

# View the points highlighted by Cook's distance.
ggplot(bike_weekday_noon, 
       aes(x = `Humidity`,
           y = `Total.Users`)) +
  geom_point() +
  geom_point(data = bike_weekday_noon[names(bike_weekday_noon_CD_hum_select), ],  #<- select the row names from cook's distance test
             aes(x = `Humidity`, y = `Total.Users`),  #<- define the x and y axes for the selected points
             color = "red",                           #<- color of the selected points
             size = 3) +                              #<- size of the selected points
  stat_smooth(method = "lm",
              color = "orange", 
              size = 1,         
              level = 0.95) + 
  expand_limits(x = 0, y = 0) +
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  theme(text = element_text(size = 20),                     
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed")) +
  labs(x = "Humidity %")

# points that it would make the most sense to remove.

# Let's remove the 3 points that are most likely aberrations that are
# skewing our model.
bike_weekday_noon_clean_2 = subset(bike_weekday_noon, 
                                   !(`Total.Users` %in% c(471, 468) |  #<- the first set of conditions "|" means "or"
                                       Humidity %in% c(0)))            #<- the second set of conditions  
View(bike_weekday_noon_clean_2)

# re-run the regression without these 3 data points.
lm_bike_weekday_noon_clean_2 = lm(`Total.Users` ~ `Humidity`,bike_weekday_noon_clean_2)

summary(lm_bike_weekday_noon_clean_2)

#  re-run the temperature model.
lm_bike_weekday_noon_clean_3 = lm(`Total.Users` ~ `Temperature.F`,bike_weekday_noon_clean_2)

summary(lm_bike_weekday_noon_clean_3)

# view the revised Total Users vs. Humidity plot. without outliers
ggplot(bike_weekday_noon_clean_2, 
       aes(x = Humidity,
           y = `Total.Users`)) +
  geom_point() +
  stat_smooth(method = "lm",
              color = "orange",
              size = 2,
              level = 0.95) +
  expand_limits(x = 0, y = 0) +
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  theme(text = element_text(size = 20),
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed")) + 
  labs(x = "Humidity %")

#  view the revised Total Users vs. Temperature plot.
ggplot(bike_weekday_noon_clean_2, 
       aes(x = `Temperature.F`,
           y = `Total.Users`)) +
  geom_point() +
  stat_smooth(method = "lm",
              color = "orange",
              size = 2,
              level = 0.95) +
  expand_limits(x = 0, y = 0) +
  scale_y_continuous(breaks = seq(0, 500, by = 50)) +
  scale_x_continuous(breaks = seq(0, 100, by = 10)) +
  theme(text = element_text(size = 20),
        panel.border = element_rect(color = "black", 
                                    fill = NA, 
                                    size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major = element_line(color = "grey", 
                                        linetype = "dashed"))
#==================================================================================

#### Multivariate regression ####

# Let's combine both temperature and humidity to predict the total number of 
# bike users. When you use several factors in your regression it's called
# multivariate regression.
multi_reg_bike_weekday_noon = lm(formula = `Total.Users` ~   #<- the "~" sign is followed by the independent variables (predictors)
                                   `Temperature.F` +         #<- the "+" sign is followed by additional independent variables (predictors)
                                   Humidity,              
                                 data = bike_weekday_noon)

summary(multi_reg_bike_weekday_noon)

# Let's try Cook's method to identify outliers.
bike_weekday_noon_CD_multi = cooks.distance(multi_reg_bike_weekday_noon)
bike_weekday_noon_CD_multi_select = bike_weekday_noon_CD_multi[bike_weekday_noon_CD_multi > 
                                                                 4/nrow(bike_weekday_noon)]
bike_weekday_noon_CD_multi_select
names(bike_weekday_noon_CD_multi_select)

# View the data identified by Cook's method as potential outliers.
# Select only the rows and columns that we're interested in.
View(bike_weekday_noon[c(names(bike_weekday_noon_CD_multi_select)),
                       c("Total.Users", "Humidity", "Temperature.F")])

##Multivariate regression: plotting ####

# Let's plot our model on a 3D plane. Let's add a column to our data that will 
# represent seasons of the year with a different color.
season_color = ifelse(bike$Season == 1, "green",
                      ifelse(bike$Season == 2, "red",
                             ifelse(bike$Season == 3, "yellow",
                                    ifelse(bike$Season == 4, "black", NA))))

# Add the new column season color
bike_season_color = cbind(bike, season_color)
View(bike_season_color)

# Subset just the data points at noon on working days.
bike_weekday_noon = subset(bike_season_color, 
                           `Working.Day` == 1 & Hour == 12)  #<- criteria for subsetting
View(bike_weekday_noon)

# Remove the outliers.  
bike_weekday_noon_clean_2 = subset(bike_weekday_noon, 
                                   !(`Total.Users` %in% c(471, 468) |  
                                       Humidity %in% c(0)))            
View(bike_weekday_noon_clean_2)

# Re-run the regression without the outliers.
multi_reg_bike_weekday_noon = lm(formula = `Total.Users` ~   
                                   `Temperature.F` +         
                                   Humidity,              
                                 data = bike_weekday_noon_clean_2)

summary(multi_reg_bike_weekday_noon)

##Multivariate regression: plotting ####

# We'll use the scatterplot3d package to plot a 3D regression.
install.packages("scatterplot3d")
library(scatterplot3d)

s3d = scatterplot3d(x = bike_weekday_noon_clean_2$`Temperature.F`, 
                    y = bike_weekday_noon_clean_2$Humidity, 
                    z = bike_weekday_noon_clean_2$`Total.Users`,  
                    xlim = c(0, 100),         #<- limits of the x-axis           
                    ylim = c(15, 100),         #<- limits of the y-axis                     
                    zlim = c(0, 500),         #<- limits of the z-axis         
                    xlab = "Temperature F",                                 
                    ylab = "Humidity %",                                  
                    zlab = "Number of Bikes Rented",                        
                    main = "Washington DC Bike Sharing Analysis",    
                    color = bike_weekday_noon_clean_2$season_color,  #<- color 
                    pch = 20,                  #<- size of the points
                    box = TRUE,                #<- display a box around the grid
                    grid = TRUE,               #<- display a grid between the x and y axes
                    #highlight.3d = TRUE,      #<- can be used to automatically create a color gradient
                    #type = "h",               #<- adds vertical lines that lead to the points, which can make it easier to interpret the graph     
                    #lty.hplot = "solid")      #<- specifies what type of line should lead up to the points if "type" is set to "h"
                    angle = 50)                #<- angle between the x and y axes

##Multivariate regression: plotting ####

# Add a best-fit plane. 
s3d$plane3d(multi_reg_bike_weekday_noon,  #<- the regression object from the multivariate regression we ran
            col = "orange",               #<- color of the lines of the plane
            lty.box = "solid")            #<- a solid border around the best-fit plane

# Add legend that identifies colors.
legend(s3d$xyz.convert(20, 100, 500),     #<- location coordinates of the legend (x, y, z)
       legend = c("Spring",               #<- labels of the legend
                  "Summer", 
                  "Fall", 
                  "Winter"),
       col = c("green",                   #<- colors associated with each label
               "red", 
               "yellow", 
               "black"),  
       cex = 0.8,                         #<- size and spacing of text
       pch = 16)                          #<- determines how the colors are displayed in the legend (points, lines, squares, etc.)

# Save 
dev.copy(pdf, "Bike Data 3D.pdf", width = 12, height = 6)
dev.off()



#==================================================================================

###Multivariate regression: adding variables ####

# excluding the 3 outliers we've omitted.
# i will try to model continuous predictors to see how well it fits the data without losing explanatory power
lm_multivariate_bike = lm(formula = `Total.Users` ~ `Temperature.F` + 
                            `Wind.Speed` + 
                            `Temperature.Feels.F` + 
                            Humidity, 
                          data = bike_weekday_noon_clean_2)

summary(lm_multivariate_bike)

# Check the standard deviation of residuals.
multivariate_bike_resid = as.data.frame(residuals(lm_multivariate_bike))
sd(unlist(multivariate_bike_resid))
#72.22

# Calculate correlation.
cor(bike_weekday_noon_clean_2$`Temperature.F`,  #<- first variable
    bike_weekday_noon_clean_2$`Total.Users`)    #<- second variable
#0.491

cor(bike_weekday_noon_clean_2$`Temperature.Feels.F`,  #<- first variable
    bike_weekday_noon_clean_2$`Total.Users`)          #<- second variable
#0.485

cor(bike_weekday_noon_clean_2$Humidity,       #<- first variable
    bike_weekday_noon_clean_2$`Total.Users`)  #<- second variable
#-0.316

cor(bike_weekday_noon_clean_2$`Wind.Speed`,   #<- first variable
    bike_weekday_noon_clean_2$`Total.Users`)  #<- second variable
#-0.080

#==================================================================================


#### Multivariate regression: adding variables ####

# Let's create a Q-Q plot to check if the residuals are normally distributed.
qqnorm(lm_multivariate_bike$residuals,
       ylab = "Observed Quantiles",
       main = "Q-Q Plot to Test Normality of Residuals")

qqline(lm_multivariate_bike$residuals,
       col = "red", 
       lwd = 3)

#==================================================================================



# Check the correlations between the variables. 
bike_clean_corr = bike_weekday_noon_clean_2[, 1:14]
colnames(bike_clean_corr) = c("Date", "Season", "Hour", "Holiday",
                              "Day_Week", "Work_Day", "Weather", "TemP_F",
                              "Temp_Feels_F", "Humidity", "Wind_Speed", 
                              "Cas_Users", "Reg_Users", "Total_Users")

pdf("Bike data correlations - clean_noon_weekday.pdf",
    width = 8,
    height = 8)

ggpairs(bike_clean_corr[, c(8:11, 14)],  
        lower = list(continuous = "smooth"), 
        diag = list(continuous = "bar"))

dev.off()

#==================================================================================


###Time-series analysis ####

# Let's get back to our bike rental data.
# Perhaps the data has a pattern not just with respect to other variables,
# but more importantly with respect to time. Perhaps over time, as people 
# become more aware of the Bikeshare program the customer base grows so
# our model also needs to take into account the effects of time.

# In order to visualize dates will need to load the "scales" package.
library(scales)

# Let's select only the data points from noon on working days.
bike_weekday_noon = subset(bike, 
                           `Working Day` == 1 & Hour == 12)  #<- criteria for subsetting
View(bike_weekday_noon)

# Let's remove the outliers we identified earlier.
bike_weekday_noon_clean_3 = subset(bike_weekday_noon, 
                                   !(`Total.Users` %in% c(471, 468) |  #<- the first set of conditions "|" means "or"
                                       Humidity %in% c(0)))            #<- the second set of conditions  
View(bike_weekday_noon_clean_3)


bike_weekday_noon_date = bike_weekday_noon_clean_3
bike_weekday_noon_date$Date = as.Date(bike_weekday_noon_date$Date, "%m/%d/%Y")  #<- month / day / year format
View(bike_weekday_noon_date)
str(bike_weekday_noon_date)

#####################################################################################################
#####################################################################################################
#######################################################################################################
### Time-series analysis ####

# Let's view the data and add a curved best-fit line. 
range(bike_weekday_noon_date$`Total Users`)

ggplot(bike_weekday_noon_date, 
       aes(x = Date,
           y = `Total.Users`)) +
  geom_point() +
  stat_smooth(method = "lm",
              se = FALSE,
              color = "orange",
              size = 3,
              formula = y ~ poly(x,                   #<- the independent variable
                                 6,                   #<- the degree of the polynomial
                                 raw = TRUE)) +       #<- use general polynomial instead of an orthogonal polynomial         
  scale_y_continuous(breaks = seq(0, 450, by = 50)) +
  scale_x_date(breaks = seq(as.Date("2011-01-01"),    #<- beginning date
                            as.Date("2013-01-01"),    #<- ending date
                            by = "2 month"),          #<- date interval
               labels = date_format("%b %Y")) +       #<- format of the labels
  theme(axis.text.x = element_text(size = 20,         #<- size of the x-axis labels
                                   angle = 45,        #<- angle of the x-axis labels
                                   vjust = 0.5),      #<- vertical alignment of the x-axis labels
        axis.text.y = element_text(size = 20),
        text = element_text(size = 20),
        panel.border = element_rect(color = "black", fill = NA, size = 1),
        panel.background = element_rect(fill = "white")) +
  labs(title = "Capital Bikeshare Users")

#==================================================================================

####  Time-series analysis: weekly ####

# Let's look at the trends of bike rentals over 4 weeks. Each week has 168 hours
# (24 * 7), so let's subset 672 data points from the data (168 * 4).
View(bike)
bike_4_weeks = bike[1:(168*4), ]

# Let's add a new column of 700 numbers so we can label the x-axis easily.
bike_4_weeks = cbind(bike_4_weeks, seq(1:672))

ggplot(bike_4_weeks, 
       aes(x = seq(1:672),
           y = `Total.Users`)) +
  geom_point() +
  geom_line(color = "orange", size = 1) +
  scale_x_continuous(breaks = seq(0, 700, by = 50)) +
  theme(text = element_text(size = 20),
        panel.border = element_rect(color = "black", fill = NA, size = 1),
        panel.background = element_rect(fill = "white"),
        panel.grid.minor = element_line(color = NA),         
        panel.grid.major.y = element_line(color = "grey", linetype = "dashed")) +
  labs(title = "Capital Bikeshare Users") +
  xlab("Hours Over 4 Weeks of January 2011")

#==================================================================================
